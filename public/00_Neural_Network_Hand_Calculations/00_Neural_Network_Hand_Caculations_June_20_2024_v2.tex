\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage[a4paper, vertical, margin=1in]{geometry} % For margins and type of paper
\usepackage{booktabs} % For better horizontal lines
\usepackage{tabularx} % For width-adjustable tables
\usepackage{amsmath} % Math typesetting
\usepackage{amsfonts} % For math symbols and fonts
\usepackage{float}    % For precise placement of tables
\usepackage{neuralnetwork}

\title{Neural Network Hand Calculations}
\author{Hanhee Lee}
\date{May 2024}

\begin{document}

\maketitle

\section{Introduction}
This mini lecture introduces a simple regression models to demonstrate the underlying working mechanics of a shallow and deep neural network.

\section{Notation}
The following table summarizes the notation used in the neural network models:

% Define custom column types with different proportional widths
\newcolumntype{A}{>{\hsize=0.5\hsize}X} % You might need to adjust these ratios
\newcolumntype{B}{>{\hsize=0.5\hsize}X}
\newcolumntype{C}{>{\hsize=2.0\hsize}X} % Adjusted for a total of 3.0\hsize

\begin{table}[htbp]
\centering
\caption{Detailed Explanation of Variables}
\label{tab:variables}
\begin{tabularx}{\textwidth}{ABC} % Custom column widths
\toprule
\textbf{Header} & \textbf{Dimension} & \textbf{Explanation} \\
\midrule
\multicolumn{3}{c}{\textbf{Superscripts}} \\
\midrule
$[l]$ & $1$ & Current layer \\
$(i)$ & $1$ & ith training example \\

\midrule
\multicolumn{3}{c}{\textbf{Subscripts}} \\
\midrule
$j$ & $1$ & jth node of the current layer \\
$k$ & $1$ & kth input into current layer \\

\midrule
\multicolumn{3}{c}{\textbf{Sizes}} \\
\midrule
$m$ & $1$ & Number of training examples in the dataset \\
$n_x$ & $1$ & Number of nodes in the input layer \\
$n_y$ & $1$ & Number of nodes in the output layer \\
$n^{[l]}$ & $1$ & Number of nodes in the current layer \\ 
$n^{[l-1]}$ & $1$ & Number of nodes in the previous layer \\ 
$L$ & $1$ & Number of layers in the network \\


\midrule
\multicolumn{3}{c}{\textbf{Objects}} \\
\midrule
$\textbf{X}$ & $n_{x}\times m$ & Input matrix \\
$\textbf{x}^{(i)}$ & $n_{x}\times 1$ & ith example represented as a column vector \\
$\textbf{W}^{[l]}$ & $n^{[l]} \times n^{[l-1]}$ & Weight matrix of the current layer \\
$z_{j}^{[l](i)}$ & $1$ & A weighted sum of the activations of the previous layer, shifted by a bias \\
$w_{j,k}^{[l]}$ & $1$ & A weight that scales the $kth$ activation of the previous layer \\
$b^{[l]}$ & $n^{[l]} \times 1$ & Bias vector in the current layer \\
$b_{j}^{[l]}$ & $n^{[l]} \times 1$ & Bias in the current layer \\
$a_{j}^{[l](i)}$ & $1$ & An activation in the current layer \\
$a_{k}^{[l-1](i)}$ & $1$ & An activation in the previous layer \\
$g_{j}^{[l]}$ & $1$ & An activation function used in the current layer \\
\bottomrule
\end{tabularx}
\end{table}

\section{Neural Network Formulas}
This section describes the foundational equations used in neural networks, detailing the computation involved in forward propagation.

\subsection*{Layer-Wise Computation}
\begin{equation}
    z^{[l]} = \sum_{k} w_{k,j} x_k + b_{l,j}
\end{equation}
\\ This equation calculates the weighted sum $z_{l,i}$ of layer \(l\), combined with the bias term $b_{l,j}$ for each neuron \(j\) in layer \(l\),  to produce the pre-activation value for neuron \(j\) in layer \(i\) for a given input \(x_k\). The term $w_{j,k}$ represents the weight for the given input \(x_k\) in neuron \(j\)
\\

\noindent \textbf{Note:}
    \begin{itemize}
        \item the initial weights are set as 1 in this example for calculation simplicity. During actual training, it's better to have random weight initialization to avoid all neurons learning the same feature, and allow the network to learn more complex patterns
    \end{itemize}
    
\begin{equation}
    L = \sqrt{\frac{1}{m} \sum_{i=1}^m (\hat{y}_i - y_i)^2} 
\end{equation}
\\ This equation represents the root mean squared error, a common cost function used to measure the difference between the predicted outputs (\(\hat{y}_i\)) and the actual targets (\(y_i\)) over all \(m\) training examples.

\section{Simple Regression Models}
To demonstrate the neural network's learning mechanism, we will use a simple regression model that is described by the following linear relationship:
\begin{equation}
    y = x_1^3 - 2x_{2}
\end{equation}

\section{Neural Network Diagram}
This section details the architecture of a simple neural network, which is diagrammed below (please ignore x0, h0, and w):
\begin{itemize}
    \item \textbf{Input Layer:} Consists of 2 nodes, representing the input features ($x_1, x_2$).
    \item \textbf{Hidden Layer:} Comprises 4 nodes, which facilitate the learning of non-linear relationships.
    \item \textbf{Output Layer:} Contains a single node, which outputs the prediction y of the network.
\end{itemize}

\begin{center}
\scalebox{1.5}{
\begin{neuralnetwork}
\newcommand{\x}[2]{$x_#2$}
\newcommand{\y}[2]{$y_#2$}
\newcommand{\h}[2]{$h_#2$}
\newcommand{\w}[4]{$w^{[1]}_{#2,#4}$}
\newcommand{\W}[4]{$w^{[2]}_#2$}
\setdefaultlinklabel{\w}
\inputlayer[count=2, text=\x]
\hiddenlayer[count=4, text=\h] 
\foreach \n in {1,2}{
    \foreach \m in {1,2,3,4}{
        \link[style={}, labelpos=near start, from layer=0, from node=\n, to layer=1, to node=\m]
    }
}
\setdefaultlinklabel{\W}
\outputlayer[count=1, text=\y] \linklayers
\end{neuralnetwork}
}
\end{center}
$x_i$ is the input data, $w_{i,j}$ is the weight in the hidden layer, $W_i$ is the weight in the output layer, and $y_1$ demonstrates the output result. 
\\

\section{Hyperparameters}
Hyperparameters are configurations external to the model that influence how the network is structured and trained. Hyperparameters play a crucial role in determining the model's performance by affecting how quickly and effectively it learns from the training data.

\begin{table}[H] % The H specifier forces the table to be placed "Here"
\centering
\caption{Detailed Explanation of Variables}
\label{tab:variables}
\begin{tabularx}{\textwidth}{AB} % Custom column widths
\toprule

\textbf{Hyperparameter} & \textbf{Description} \\
\midrule
Number of hidden layers & $1$ \\
Optimizer & Gradient descent backpropagation \\
Number of nodes in the hidden layer & 4 \\ 
Activation function of the hidden layer & ReLU function \\
Activation function of the output layer & Sigmoid function \\
Loss function & Root mean squared error \\ 
Learning rate & 1 \\ 
Number of epochs & 1 \\ 

\bottomrule
\end{tabularx}
\end{table}

\section{Example Process: Mapping Function in Neural Network}
Now we are going to work through the process how this neural network is trained with specific inputs and expected output. The inputs for this example are chosen with feature dimensions of 2 and 1, respectively.

\vspace{5mm}

\noindent \textbf{Input Layer:} The network receives a single training point with features:
    \begin{itemize}
        \item $x_1 = 2$ 
        \item $x_2 = 1$ 
    \end{itemize}
    These inputs are fed into the network to process through the neural architecture.

\section{Forward Propagation}
Forward Propagation involves processing input data through the network from the input to the output layer using current weights and biases, generating predictions that are used to calculate the error against actual targets.

\begin{enumerate}



    \item \textbf{Input to Hidden Layer:}
    \begin{flalign*}
        \text {As indicated before, we have the input vector X as a 2x1 matrix \textbf{X} =}\begin{bmatrix} x_1 \\ x_2  \end{bmatrix}, \text{ where $x_1$ = 2 and $x_2$ = 1} &
    \end{flalign*}
    \begin{itemize}
        \item Here, the input vector \textbf{X} $=\begin{bmatrix} 2 \\ 1 \end{bmatrix}$to the network consists of data points that we want the network to learn from ($x_1$ and $x_2$)
    \end{itemize}
    
    \begin{flalign*}
        \text{Let } & \textbf{W}_{1} = \begin{bmatrix} 
            w^{[1]}_{1,1} & w^{[1]}_{2,1} \\ 
            w^{[1]}_{1,2} & w^{[1]}_{2,2} \\ 
            w^{[1]}_{1,3} & w^{[1]}_{2,3}\\
            w^{[1]}_{1,4} & w^{[1]}_{2,4}\\
        \end{bmatrix}, \text{ be the weight matrix connecting the input to the hidden layer}  &
    \end{flalign*}
    \begin{itemize}
        \item $\textbf{W}^{[1]}$ is the weight matrix associated with the first layer. Each element, such as $w_{1,1}$, represents the weight connecting the input node $x_1$ to the first neuron in the hidden layer.
    \end{itemize}

    \begin{flalign*}
        \text{Let } & \textbf{b}^{[1]} = \begin{bmatrix} b_{1,1} \\ b_{1,2} \\ b_{1,3} \\ b_{1,4} \end{bmatrix}, \text{ for the hidden layer.} &
    \end{flalign*}
    \begin{itemize}
        \item $\textbf{b}^{[1]}$ represents the bias vector for the hidden layer, with each entry like $b_{1,1}$ adding a bias term to the corresponding neuron's output. This helps to adjust the threshold at which the neuron activates.
    \end{itemize}
    
    \begin{flalign*}
        & \textbf{z}^{[1]} = \textbf{W}^{[1]} \textbf{x} + \textbf{b}^{[1]} &
    \end{flalign*}
    \begin{itemize}
        \item This equation computes the linear combination of inputs and weights, adjusted by the bias. The result, $\textbf{z}^{[1]}$, is the pre-activation output of the hidden layer.
    \end{itemize}
    
    \begin{flalign*}
        & \begin{bmatrix} z_{1,1}\\ z_{1,2} \\ z_{1,3} \\ z_{1,4} \end{bmatrix} = \begin{bmatrix} 
            w^{[1]}_{1,1} & w^{[1]}_{2,1} \\ 
            w^{[1]}_{1,2} & w^{[1]}_{2,2} \\ 
            w^{[1]}_{1,3} & w^{[1]}_{2,3}\\
            w^{[1]}_{1,4} & w^{[1]}_{2,4}\\
        \end{bmatrix} 
        \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} +
        \begin{bmatrix} b_{1,1} \\ b_{1,2} \\ b_{1,3} \\ b_{1,4} \end{bmatrix} &\\
        %-------------------------------------------
        & \begin{bmatrix} z_{1,1} \\ z_{1,2} \\ z_{1,3} \\ z_{1,4} \end{bmatrix} = \begin{bmatrix} 
            1 & 1 \\ 
            1 & 1 \\ 
            1 & 1 \\ 
            1 & 1 \\ 
        \end{bmatrix} 
        \begin{bmatrix} 2 \\ 1 \end{bmatrix} +
        \begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \end{bmatrix} &
    \end{flalign*}
    \begin{itemize}
        \item This step shows the explicit matrix multiplication and addition for the given example. It is a practical computation where each neuron's input is the sum of products of each input feature and the corresponding weight plus a bias term.
    \end{itemize}
    \begin{flalign*}
        & \textbf{z}^{[1]} =  \begin{bmatrix} 3 \\ 3 \\ 3 \\ 3 \end{bmatrix} &
    \end{flalign*}
    \begin{itemize}
        \item The result is a vector of pre-activation values for each neuron in the hidden layer.
    \end{itemize}



    \item \textbf{Activation in Hidden Layer:}
    \begin{flalign*}
        & \textbf{a}^{[1]} = \text{ReLU}(\textbf{z}^{[1]}) & 
    \end{flalign*}
    \begin{itemize}
        \item The ReLU function is applied to each pre-activation value. This non-linear function outputs the input directly if it is positive; otherwise, it outputs zero.
    \end{itemize}
    
    \begin{flalign*}
        & \begin{bmatrix} a_{1,1} \\ a_{1,2} \\ a_{1,3} \\ a_{1,4} \end{bmatrix} = \text{ReLU}\left(\begin{bmatrix} 3 \\ 3 \\ 3 \\ 3 \end{bmatrix}\right) &
    \end{flalign*}
    \begin{itemize}
        \item Since all inputs are positive (6), the ReLU output matches the input. This vector represents the activated output of the hidden layer.
    \end{itemize}
    \begin{flalign*}
        & \begin{bmatrix} z_{2,1}
        \end{bmatrix} = \begin{bmatrix} 
            w^{[1]}_{1,1} & w^{[1]}_{2,1} \\ 
            w^{[1]}_{1,2} & w^{[1]}_{2,2} \\ 
            w^{[1]}_{1,3} & w^{[1]}_{2,3}\\
            w^{[1]}_{1,4} & w^{[1]}_{2,4}\\
        \end{bmatrix} 
        \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} +
        \begin{bmatrix} b_{1,1} \\ b_{1,2} \\ b_{1,3} \\ b_{1,4} \end{bmatrix} &\\
        %-------------------------------------------
        & \begin{bmatrix} z_{1,1} \\ z_{1,2} \\ z_{1,3} \\ z_{1,4} \end{bmatrix} = \begin{bmatrix} 
            1 & 1 \\ 
            1 & 1 \\ 
            1 & 1 \\ 
            1 & 1 \\ 
        \end{bmatrix} 
        \begin{bmatrix} 2 \\ 1 \end{bmatrix} +
        \begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \end{bmatrix} &
    \end{flalign*}
    \begin{flalign*}
        & \textbf{a}^{[1]} = \begin{bmatrix} 3 \\ 3 \\ 3 \\ 3 \end{bmatrix} &
    \end{flalign*}
    \begin{itemize}
        \item These are the activation values from the hidden layer that will be fed to the next layer.
    \end{itemize}



    \item \textbf{Hidden Layer to Output Layer:}
    \begin{flalign*}
        & \text{Let } \textbf{W}^{[2]} = \begin{bmatrix} w_{1,1}^{[2]} & w_{1,2}^{[2]} & w_{1,3}^{[2]} & w_{1,4}^{[2]} \end{bmatrix} \text{and } \textbf{b}^{[2]} = \begin{bmatrix} b_{2,1} \end{bmatrix} &
    \end{flalign*}
    \begin{itemize}
        \item $\textbf{W}^{[2]}$ and $\textbf{b}^{[2]}$ are the weight vector and bias for the output layer. Here, we're transitioning from a hidden layer with multiple neurons to an output layer with potentially one neuron.
    \end{itemize}
    \begin{flalign*}
        & \textbf{z}^{[2]} = \textbf{W}^{[2]} \textbf{a}^{[1]} + \textbf{b}^{[2]} &
    \end{flalign*}
    \begin{itemize}
        \item This equation calculates the linear combination of the activated outputs from the hidden layer, weighed by $\textbf{W}^{[2]}$, and adjusted by the bias $\textbf{b}^{[2]}$. The result is the input to the output layer's activation function.
    \end{itemize}
    
    \begin{flalign*}
        & \begin{bmatrix} z_{2,1} \end{bmatrix} = \begin{bmatrix} w_{1,1}^{[2]} & w_{1,2}^{[2]} & w_{1,3}^{[2]} & w_{1,4}^{[2]} \end{bmatrix}
        \begin{bmatrix} 3 \\ 3 \\ 3 \\ 3 \end{bmatrix} +
        \begin{bmatrix} b_{2,1} \end{bmatrix} &\\
        %-------------------------------------------
        & \begin{bmatrix} z_{2,1} \end{bmatrix} = \begin{bmatrix} 
            1 & 1 & 1 & 1
        \end{bmatrix} 
        \begin{bmatrix} 3 \\ 3 \\ 3 \\ 3 \end{bmatrix} +
        \begin{bmatrix} 0 \end{bmatrix} &
    \end{flalign*}
    \begin{flalign*}
        & \textbf{z}^{[2]} = \begin{bmatrix} 12 \end{bmatrix} &
    \end{flalign*}
  

    \item \textbf{Activation in Output Layer:}
    \begin{flalign*}
        & a^{[2]} = \hat{y} = \sigma(\textbf{z}^{[2]}) &
    \end{flalign*}
    \begin{itemize}
        \item The sigmoid function $\sigma$ is used at the output layer to map the input value into a (0,1) range, which is typical for binary classification tasks or probability estimation.
    \end{itemize}
    \begin{equation}
        \sigma(x) = \frac{1}{1+e^{-x}}
    \end{equation}
    
    \begin{flalign*}
        & \hat{y} = \sigma(\begin{bmatrix} 12 \end{bmatrix}) = 0.99 &
    \end{flalign*}
    \begin{itemize}
        \item Given the high input value of 12, the sigmoid function outputs a value close to 1, indicating a high confidence level in the positive class, assuming a binary classification context.
    \end{itemize}


    
\end{enumerate}

\section{Backward Propogation}
Backward Propagation adjusts the networkâ€™s parameters by calculating the loss function's gradient and updating weights and biases to minimize prediction errors, optimizing network performance over training iterations.

\begin{enumerate}
    \item \textbf{Calculate Loss:}
    \begin{flalign*}
        & y_i = x_1^3 - 2x_2 &\\
        & y_i = 2^3 - 2 &\\
        & y_i = 6 &\\
        & E = \sqrt{\frac{1}{m} \sum_{i=1}^m (\hat{y}_i - y_i)^2} & \\ 
        & E = \sqrt{(0.99 - 6)^2} = \sqrt{25.1001} & \\
        & E = 5.01 &
    \end{flalign*}
    \begin{itemize}
        \item This formula calculates the Root Mean Squared Error (RMSE) between the predicted values (\(\hat{y}_i\)) and the actual values (\(y_i\)). Here, \(\hat{y}_i = 0.99\) and \(y_i = 6\).
        \item The RMSE provides a measure of how well the model is predicting the output, quantifying the difference in terms of the model's accuracy. In this case, an RMSE of 11.01 indicates a significant error, showing that the model's prediction is far from the actual value.
    \end{itemize}
    
    \item \textbf{Output to Hidden Layer:}
    \begin{flalign*}
        & \text{Using the chain rule, we first calculate the gradient of the loss function with respect to the output predictions:} & \\
        & \frac{\partial L}{\partial \hat{y}} = \frac{2}{m} (\hat{y} - y) \frac{1}{2\sqrt{\text{mean squared error}}} & \\
        & \text{For the next layer's pre-activation output, we derive the gradient with respect to the sigmoid function:} & \\
        & \frac{\partial L}{\partial z^{[2]}} = \frac{\partial L}{\partial \hat{y}} \cdot \sigma'(z^{[2]}) &
    \end{flalign*}
    \begin{itemize}
        \item \( \sigma'(z^{[2]}) \) is the derivative of the sigmoid activation function, applied to the pre-activation outputs at the output layer.
    \end{itemize}

    \item \textbf{Hidden Layer to Input Layer:}
    \begin{flalign*}
        & \text{The gradients of the weights and biases are calculated as follows:} & \\
        & \frac{\partial L}{\partial W^{[2]}} = \frac{\partial L}{\partial z^{[2]}} \cdot a^{[1]} & \\
        & \frac{\partial L}{\partial b^{[2]}} = \frac{\partial L}{\partial z^{[2]}} & \\
        & \text{For the activations of the previous layer, we use the transpose of the weights:} & \\
        & \frac{\partial L}{\partial a^{[1]}} = W^{[2]T} \cdot \frac{\partial L}{\partial z^{[2]}} & \\
        & \text{The derivative through the ReLU function is computed next:} & \\
        & \frac{\partial L}{\partial z^{[1]}} = \frac{\partial L}{\partial a^{[1]}} \cdot \text{ReLU}'(z^{[1]}) &
    \end{flalign*}
    \begin{itemize}
        \item \( \text{ReLU}'(z^{[1]}) \) is the derivative of the ReLU function, which is 1 for positive inputs and 0 otherwise.
    \end{itemize}

    \item \textbf{Update Parameters:}
    \begin{flalign*}
        & \text{Finally, we update the weights and biases using the calculated gradients and a learning rate \( \alpha \):} & \\
        & W^{[l]} = W^{[l]} - \alpha \cdot \frac{\partial L}{\partial W^{[l]}} & \\
        & b^{[l]} = b^{[l]} - \alpha \cdot \frac{\partial L}{\partial b^{[l]}} &
    \end{flalign*}
    \begin{itemize}
        \item These updates adjust the weights and biases to minimize the loss, thereby improving the model with each iteration.
    \end{itemize}
\end{enumerate}

\section{References}

\end{document}